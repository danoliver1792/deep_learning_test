{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXBdO63sZeGArwLghSvY/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danoliver1792/deep_learning_test/blob/main/rede_neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3ErBHfu-G0yj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #Define a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data', download=True, train=True, transform=transform) #Carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por parte\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data', download=True, train=False, transform=transform) #Carrega a parte de validação\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "3j2UQq1-G-km"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pegando uma imagem da base de dados para visualização\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sYzDFFCII9pa",
        "outputId": "8b84a6b9-bc5c-49f5-8d66-a0f8f51b3567"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #Para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) #Para verificar as dimensões dp tensor de cada etiqueta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5ot1nZTJlzJ",
        "outputId": "5ccbe912-a848-4842-e334-13d9df1b36e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) #Camada de entrada, 784 neuronios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) #Camada interna 1, 128 neuronios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) #Camada interna 2, 64 neuronios que se ligam a 10\n",
        "\n",
        "    #Para a camada de saída não é necessário definir nada, porque só precisamps pegar o output da camada interna 2\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X)) #Função de ativação da camada de entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) #Função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) #Função de ativação da camada 2 para a camada de saída, nesse caso f(x) = x\n",
        "    return F.log_softmax(X, dim=1) #Dados utilizados para calcular a perda "
      ],
      "metadata": {
        "id": "_8ocSSjwK-If"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "  otimizador = optim.SGC(modelo.parameters(), lr=0.01, momentum=0.5) #Define a política de atualização de pesos e da bias\n",
        "  inicio = time() #Timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLoss() #Definindo o critério para calcular a perda\n",
        "  EPOCHS = 10 #Numero de epochs que o algoritmo rodará\n",
        "  modelo.train() #Ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 #Inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "      imagens = imagens.view(imagens.shape[0], -1) #Convertendo as imagens para \"vetores\" de 28*28 casas\n",
        "      otimizador.zero_grad() #Zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) #Colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) #Calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() #Back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() #Atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() #Atualização da perda acumulada\n",
        "\n",
        "    else:\n",
        "      print('Epoch {} - Perda resultante: {}'.format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print('\\nTempo de treino (em minutos) =', (time()-inicio)/60)"
      ],
      "metadata": {
        "id": "VC70YJw1R3ea"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      #Desativas o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) #Output do modelo em escala logaritmica\n",
        "\n",
        "      ps = torch.exp(logps) #Converte output para escala normal (lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) #Converte o tensor em um número, no caso, o número que o modelo previu como correto\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if etiqueta_certa == etiqueta_pred:\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "\n",
        "    print('Total de imagens testadas =', conta_todas)\n",
        "    print('\\nPrecisão do modelo = {}%'.format(conta_corretas * 100 / conta_todas))\n",
        "    "
      ],
      "metadata": {
        "id": "zzLZ8WRxUVUQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #Verificando se tem o cuda disponível para nossa GPU\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aruBXAE3WM3q",
        "outputId": "4256c8dd-5979-44ac-9134-5cc02ffdadbc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}